<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python爬虫笔记 on JadeRong</title>
    <link>https://rrrwx.github.io/categories/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in python爬虫笔记 on JadeRong</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 May 2020 00:23:18 +0800</lastBuildDate>
    
	<atom:link href="https://rrrwx.github.io/categories/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>python爬虫笔记（六）</title>
      <link>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E5%85%AD/</link>
      <pubDate>Wed, 06 May 2020 00:23:18 +0800</pubDate>
      
      <guid>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E5%85%AD/</guid>
      <description>（一）正则式表达 可参考 http://www.runoob.com/regexp/regexp-syntax.html
import re match = re.search(r&amp;#39;[1-9]\d{5}&amp;#39;, &amp;#39;BIT 200300&amp;#39;) # regex = re.compile(r&amp;#39;[1-9]\d{5}&amp;#39;) # match = regex.search(&amp;#39;BIT 200300&amp;#39;) if match: print(match.group(0)) for match in re.finditer(r&amp;#39;[1-9]\d{5}&amp;#39;, &amp;#39;BIT100081, TSU100084&amp;#39;): if match: print(match.group(0)) 200300 100081 100084 Re库默认使用贪婪匹配，即输出匹配长度最长的字符串
(二)Scrapy爬虫框架  Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。
Scrapy 常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。
通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。
 scrapy是使用命令行来爬取网站数据。首先建立一个工程项目，然后在spider目录下编写爬虫代码
$ scrapy startproject spiderdemo $ cd spiderdemo $ scrapy genspider demo python123.io 这样就可以在spider目录下看到demo.py，然后往里面添加爬虫需求即可。
# -*- coding: utf-8 -*- import scrapy class DemoSpider(scrapy.Spider): name = &amp;#34;demo&amp;#34; #allowed_domains = [&amp;#34;python123.</description>
    </item>
    
    <item>
      <title>python爬虫笔记（五）</title>
      <link>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E4%BA%94/</link>
      <pubDate>Tue, 05 May 2020 00:22:18 +0800</pubDate>
      
      <guid>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E4%BA%94/</guid>
      <description>(一)BeautifulSoup库 Beautiful Soup是python的一个库，主要功能是从网页抓取数据，也叫beaultifulsoup4或bs4。
 Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。
 import requests r = requests.get(&amp;#34;https://python123.io/ws/demo.html&amp;#34;) demo = r.text from bs4 import BeautifulSoup soup = BeautifulSoup(demo, &amp;#34;html.parser&amp;#34;) print(soup.prettify()) &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt; This is a python demo page &amp;lt;/title&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;p class=&amp;#34;title&amp;#34;&amp;gt; &amp;lt;b&amp;gt; The demo python introduces several python courses. &amp;lt;/b&amp;gt; &amp;lt;/p&amp;gt; &amp;lt;p class=&amp;#34;course&amp;#34;&amp;gt; Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses: &amp;lt;a class=&amp;#34;py1&amp;#34; href=&amp;#34;http://www.</description>
    </item>
    
    <item>
      <title>python爬虫笔记（三）</title>
      <link>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E4%B8%89/</link>
      <pubDate>Sun, 03 May 2020 00:21:18 +0800</pubDate>
      
      <guid>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E4%B8%89/</guid>
      <description>爬虫实例 （一）更换用户代理 爬取时默认user-agent是 * python-requests/2.18.4 *
import requests def getHTMLText(url): try: r = requests.get(url, timeout=30) print(r.request.headers) r.raise_for_status() r.encoding = r.apparent_encoding return r.text[500:800] except: return &amp;#34;wrong connection...&amp;#34; if __name__ == &amp;#34;__main__&amp;#34;: this_url = &amp;#34;http://www.amazon.cn/gp/product/B01M8L5Z3Y&amp;#34; print(getHTMLText(this_url)) {&amp;#39;User-Agent&amp;#39;: &amp;#39;python-requests/2.18.4&amp;#39;, &amp;#39;Accept-Encoding&amp;#39;: &amp;#39;gzip, deflate&amp;#39;, &amp;#39;Accept&amp;#39;: &amp;#39;*/*&amp;#39;, &amp;#39;Connection&amp;#39;: &amp;#39;keep-alive&amp;#39;} {&amp;#39;user-agent&amp;#39;: &amp;#39;Mozilla/5.0&amp;#39;, &amp;#39;Accept-Encoding&amp;#39;: &amp;#39;gzip, deflate&amp;#39;, &amp;#39;Accept&amp;#39;: &amp;#39;*/*&amp;#39;, &amp;#39;Connection&amp;#39;: &amp;#39;keep-alive&amp;#39;} ue = {}; (function(d){var e=d.ue=d.ue||{},f=Date.now||function(){return+new Date};e.d=function(b){return f()-(b?0:d.ue_t0)};e.stub=function(b,a){if(!b[a]){var c=[];b[a]=function(){c.push([c.slice.call(arguments),e.d(),d.ue_id])};b[a].replay=function(b){for(var a;a=c.shift();)b(a[0],a[1],a[2])};b[a]. 有可能会被网站根据user-agent识别出是爬虫而不是普通用户，然后可以通过更改user-agent来爬取
kv = {&amp;#39;user-agent&amp;#39;:&amp;#39;Mozilla/5.0&amp;#39;} r = requests.get(url, headers = kv) print(r.</description>
    </item>
    
    <item>
      <title>python爬虫笔记（二）</title>
      <link>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E4%BA%8C/</link>
      <pubDate>Sat, 02 May 2020 00:20:18 +0800</pubDate>
      
      <guid>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E4%BA%8C/</guid>
      <description>网络爬虫引发的问题 1.网络爬虫尺寸 （1）爬取网页 小规模、数据量小、爬取速度不敏感&amp;mdash;&amp;mdash;Requests库
（2）爬取网站、系列网站 中规模、数据规模较大、爬取速度敏感&amp;mdash;&amp;mdash;-Scrapy库
（3）爬取全网 大规模、搜索引擎、爬取速度关键&amp;mdash;&amp;mdash;&amp;ndash;定制开发
2.法律风险、骚扰问题、隐私泄露 审查来源
Robots协议：Robots Exclusion Standard网络爬虫排除标准
​ 在网站根目录下的robots.txt文件
User-agent: *
Disallow: /</description>
    </item>
    
    <item>
      <title>python爬虫笔记（一）</title>
      <link>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
      <pubDate>Fri, 01 May 2020 00:19:18 +0800</pubDate>
      
      <guid>https://rrrwx.github.io/2020/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
      <description>http://www.icourse163.org/course/BIT-1001870001?tid=1001962001#/info
授课老师：嵩天
课程简介：“The website is the API.”网络爬虫逐渐成为自动获取网络信息的主要形式。
 Requests库的使用：
import requests r = requests.get(url) 构造一个向服务器请求资源的Request对象（大写）；
返回一个包含服务器资源的Response对象；
（1）Response对象的属性
 r.status_code(200表示连接成功)
r.text（url响应内容）
r.content（响应内容的二进制形式）
r.encoding（从头部预测的编码方式）
r.apparent_encoding（从内容分析的编码方式）
&amp;mdash;&amp;mdash;-r.encoding = r.apparent_encoding
 (2)通用框架
import requests def getHTMLText(url): try: r = requests.get(url, timeout=30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return &amp;#34;wrong connection...&amp;#34; if __name__ == &amp;#34;__main__&amp;#34;: this_url = &amp;#34;http://www.baidu.com&amp;#34; print(getHTMLText(this_url)) (3)Requests库的主要方法
requests.request() #---------基础方法 requests.get();
requests.head();
requests.post();
requests.put();
requests.patch();
requests.delete()</description>
    </item>
    
  </channel>
</rss>